exports.name="lake_export";exports.version="0.0.1";exports.disabled=false;exports.handleSignals=true;exports.group=C.INTERNAL_FUNCTION_GROUP;let dataset,searchJobId,tee=false,flushMs=1e4,destination,destMeta,lastFlush,logger,_eventsOut=0,_deltaEventsMissingFields=0,_eventsMissingFields=0,_bytesOut=0,countEventsWithMissingFields=false,suppressPreviews=false,receivedEvents=false,mock;exports.init=async e=>{const t=e.conf;({dataset,searchJobId,mock}=t);if(dataset==null){if(!searchJobId)throw new Error("searchJobId field is required")}logger=C.util.getLogger(`lakeExportFunction:${searchJobId}`);suppressPreviews=t.suppressPreviews;tee=t.tee??tee;flushMs=t.flushMs??flushMs;const s=await C.internal.kusto.lake.createLakeOutput(dataset,searchJobId,logger,mock);if(s==null||s.destination==null){throw new Error(`Couldn't export results to lake dataset '${dataset}'`)}destination=s.destination;destMeta=s.destMeta;countEventsWithMissingFields=destMeta&&destMeta.acceleratedFields&&destMeta.acceleratedFields.length>0;_eventsOut=0;_bytesOut=0;_eventsMissingFields=0;logger.info("Initialized lake export",{dataset})};exports.process=async e=>{if(!tee&&e.__signalEvent__){switch(e.__signalEvent__){case"close":case"reset":case"complete_gen":return;case"timer":return flushStats(e);case"final":{if(e.__ctrlFields.includes("cancel"))return e;logger.debug("Final flushing stats & destination");await destination.flush();await destination.close();const t=createStatsEvent(e,destination,true);t.status="Exporting complete";const s=[e];if(_eventsMissingFields>0){const n=Object.assign(e.__clone(true,[]),{searchJobId,status:`Warning: ${_eventsMissingFields} events are missing accelerated fields - you should define the fields for performance purposes. Please refer to https://docs.cribl.io/lake/datasets/#accelerated-fields for more details on accelerated fields.`,_time:Date.now()/1e3});s.push(n,t)}else{s.push(t)}return s}default:logger.warn("unhandled signal",{event:e});return e}}if(countEventsWithMissingFields){for(const t of destMeta.acceleratedFields){if(!Object.hasOwn(e,t)){_deltaEventsMissingFields++;break}}}return await send(e,destination,tee)};exports.unload=()=>{destination=undefined;dataset=undefined;receivedEvents=false};async function send(e,t,s=false){if(s){const s=e.__clone();await t.send(s);return e}await t.send(e);return flushStats(e)}function createStatsEvent(e,t,s=false){const n=t.reportStatus();const a=e.__clone(true,[]);const{sentCount:i,bytesWritten:r}=n.metrics;const o=i-_eventsOut;const l=r-_bytesOut;if(countEventsWithMissingFields){_eventsMissingFields+=_deltaEventsMissingFields}if(!s&&l===0&&o===0)return undefined;const d=Object.assign(a,{searchJobId,eventsOut:o,bytesOut:l,totalEventsOut:i,totalBytesOut:r,status:"Exporting",_time:Date.now()/1e3});_eventsOut=i;_bytesOut=r;return d}function flushStats(e){const t=Date.now();if(!receivedEvents){const t=[];receivedEvents=true;const s=e.__clone(true,[]);const n=Object.assign(s,{searchJobId,status:"Begin exporting",_time:Date.now()/1e3});t.push(n);if(e.__signalEvent__){t.push(e)}return t}if(!lastFlush)lastFlush=t;if(suppressPreviews||lastFlush+flushMs>=t){if(e.__signalEvent__){return e}return undefined}else{const s=[];if(e.__signalEvent__){s.push(e)}const n=createStatsEvent(e,destination);lastFlush=t;n&&s.push(n);return s.length?s:undefined}}exports.UT_getDestination=()=>destination;